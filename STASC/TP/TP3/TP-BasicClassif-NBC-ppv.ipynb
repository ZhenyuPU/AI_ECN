{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation de méthodes élémentaires pour la classification supervisée : Naive Bayes et classifieur par plus proches voisins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ce TP, nous aurons besoin des modules Python ci-dessous, il vous faut donc évidemment exécuter cette première cellule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jeu de données [Vertebral Column](https://archive.ics.uci.edu/ml/datasets/Vertebral+Column) permet d'étudier les pathologies d'hernie discale et de Spondylolisthesis. Ces deux pathologies sont regroupées dans le jeu de données en une seule catégorie dite `Abnormale`. \n",
    "\n",
    "Il s'agit donc d'un problème de classification supervisée à deux classes :\n",
    "- Normale (NO) \n",
    "- Abnormale (AB)    \n",
    "\n",
    "avec 6 variables bio-mécaniques disponibles (features).\n",
    "\n",
    "L'objectif du TP est d'implémenter quelques méthodes simples de classification supervisée pour ce problème."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Télécharger le fichier column_2C.dat depuis le site de l'UCI à [cette adresse](https://archive.ics.uci.edu/ml/datasets/Vertebral+Column). \n",
    ">\n",
    "> On peut importer les données sous python par exemple avec la librairie [pandas](https://pandas.pydata.org/pandas-docs/stable/10min.html). Vous pourrez au besoin consulter la documentation de la fonction [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html). \n",
    "> \n",
    "> Le chemin donné dans la fonction `read_csv`est une chaîne de caractère qui spécifie le chemin complet vers le ficher sur votre machine. On peut aussi donner une adresse url si le fichier est disponible en ligne.\n",
    ">\n",
    "> Attention à la syntaxe pour les chemins sous Windows doit etre de la forme  `C:/truc/machin.csv`. \n",
    "> \n",
    "> Voir ce [blog](https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f) pour en savoir plus sur la \"manipulation des chemins\" sur des OS variés. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path= \"E:/Centrale Nantes/AI/A2/STASC/TP/TP3/vertebral+column/column_2C.dat\"\n",
    "Vertebral = pd.read_csv(file_path,\n",
    "                          delim_whitespace = True,\n",
    "                          header = 20)\n",
    "\n",
    "Vertebral.columns = [\"pelvic_incidence\", \n",
    "                     \"pelvic_tilt\", \n",
    "                     \"lumbar_lordosis_angle\", \n",
    "                     \"sacral_slope\",\"pelvic_radius\",\n",
    "                     \"degree_spondylolisthesis\",\n",
    "                     \"class\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Vérifier à l'aide des méthodes `.head()`  et `describe()` que les données sont bien importées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pelvic_incidence  pelvic_tilt  lumbar_lordosis_angle  sacral_slope  \\\n",
      "count        289.000000   289.000000             289.000000    289.000000   \n",
      "mean          61.383875    17.712145              53.099827     43.672145   \n",
      "std           17.271718    10.225877              18.523934     13.478980   \n",
      "min           26.150000    -6.550000              14.000000     13.370000   \n",
      "25%           47.740000    10.540000              39.000000     33.930000   \n",
      "50%           60.040000    16.480000              51.000000     43.000000   \n",
      "75%           74.430000    22.230000              64.000000     53.130000   \n",
      "max          129.830000    49.430000             125.740000    121.430000   \n",
      "\n",
      "       pelvic_radius  degree_spondylolisthesis  \n",
      "count     289.000000                289.000000  \n",
      "mean      118.108616                 28.000069  \n",
      "std        13.428337                 38.317024  \n",
      "min        70.080000                -11.060000  \n",
      "25%       110.710000                  1.740000  \n",
      "50%       118.360000                 20.320000  \n",
      "75%       125.660000                 42.890000  \n",
      "max       163.070000                418.540000  \n"
     ]
    }
   ],
   "source": [
    "Vertebral.head()\n",
    "\n",
    "summary = Vertebral.describe()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Les librairies de Machine Learning telles que `sckitlearn` prennent en entrée des tableau numpy (pas des objets pandas). Créer un tableau numpy que vous nommerez `VertebralVar` pour les features et un vecteur numpy `VertebralClas` pour la variable de classe. Voir par exemple [ici](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_numpy.html#pandas.DataFrame.to_numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VertebralVar  = Vertebral[[\"pelvic_incidence\", \n",
    "                     \"pelvic_tilt\", \n",
    "                     \"lumbar_lordosis_angle\", \n",
    "                     \"sacral_slope\",\"pelvic_radius\",\n",
    "                     \"degree_spondylolisthesis\"]].to_numpy()\n",
    "\n",
    "VertebralClas = Vertebral[\"class\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Découpage train / test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En apprentissage statistique, classiquement un prédicteur est ajusté sur une partie seulement des données et l'erreur de ce dernier est ensuite évaluée sur une autre partie des données disponibles. Ceci permet de ne pas utiliser les mêmes données pour ajuster et évaluer la qualité d'un prédicteur. Cette problématique est l'objet du prochain chapitre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> En utilisant la fonction [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) de la librairie [`sklearn.model_selection`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection), sélectionner aléatoirement 60% des observations pour l'échantillon d'apprentissage et garder le reste pour l'échantillon de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "VertebralVar_train,VertebralVar_test,VertebralClas_train, VertebralClas_test = train_test_split(VertebralVar, VertebralClas, test_size=0.4, random_state=42)\n",
    "ntot = len(VertebralVar)     # Calcule la longueur totale de l'échantillon complet (ntot)\n",
    "ntrain = len(VertebralVar_train)     # Calcule la longueur de l'échantillon d'apprentissage (ntrain)\n",
    "ntest = len(VertebralVar_test)### longueur totale de l'échantillon de test -TO DO ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque : on peut aussi le faire à la main avec la fonction [`sklearn.utils.shuffle`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.shuffle.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction des deux classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Extraire les deux sous-échantillons de classes respectives \"Abnormale\" et \"Normale\" pour les données d'apprentissage et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AB' 'AB' 'AB' 'NO' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'NO' 'NO'\n",
      " 'AB' 'AB' 'AB' 'NO' 'NO' 'AB' 'AB' 'AB' 'NO' 'AB' 'NO' 'AB' 'AB' 'AB'\n",
      " 'NO' 'AB' 'AB' 'AB' 'AB' 'NO' 'AB' 'AB' 'AB' 'NO' 'NO' 'AB' 'NO' 'NO'\n",
      " 'AB' 'AB' 'NO' 'AB' 'AB' 'AB' 'NO' 'AB' 'NO' 'NO' 'AB' 'AB' 'AB' 'AB'\n",
      " 'NO' 'AB' 'NO' 'AB' 'NO' 'NO' 'AB' 'NO' 'AB' 'AB' 'AB' 'AB' 'AB' 'NO'\n",
      " 'NO' 'AB' 'AB' 'AB' 'AB' 'NO' 'NO' 'AB' 'NO' 'AB' 'AB' 'NO' 'AB' 'AB'\n",
      " 'AB' 'NO' 'AB' 'AB' 'NO' 'NO' 'AB' 'NO' 'AB' 'NO' 'NO' 'NO' 'AB' 'AB'\n",
      " 'AB' 'NO' 'NO' 'NO' 'AB' 'AB' 'NO' 'AB' 'NO' 'NO' 'NO' 'AB' 'AB' 'AB'\n",
      " 'AB' 'AB' 'NO' 'AB' 'AB' 'AB' 'AB' 'NO' 'AB' 'NO' 'AB' 'AB' 'NO' 'NO'\n",
      " 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'NO' 'NO' 'AB' 'AB' 'AB' 'AB' 'AB'\n",
      " 'AB' 'NO' 'AB' 'NO' 'NO' 'AB' 'AB' 'AB' 'NO' 'AB' 'AB' 'AB' 'AB' 'NO'\n",
      " 'NO' 'AB' 'AB' 'NO' 'NO' 'AB' 'AB' 'AB' 'AB' 'AB' 'NO' 'AB' 'NO' 'AB'\n",
      " 'AB' 'AB' 'AB' 'NO' 'AB']\n"
     ]
    }
   ],
   "source": [
    "ab_index = (VertebralClas_train == 'AB')\n",
    "no_index = (VertebralClas_train == 'NO')\n",
    "VertebralVar_train_AB = VertebralVar_train[ab_index]\n",
    "VertebralVar_train_NO = VertebralVar_train[no_index]\n",
    "print(VertebralClas_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "n_AB = len(VertebralVar_train_AB)\n",
    "n_NO = len(VertebralVar_train_NO)\n",
    "print(n_AB)\n",
    "print(n_NO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ajuster un classifieur naif bayesien sur les données d'apprentissage.\n",
    "\n",
    "Pour une observation $x \\in \\mathbb R^6$, la régle du MAP consiste à choisir la catégorie $\\hat y (x) = \\hat k $ qui maximise (en $k$) \n",
    "$$ score_k(x) = \\hat \\pi_k \\prod_{j=1} ^6  \\hat f_{k,j}(x_j)   $$\n",
    "où :\n",
    "- $k$ est le numéro de la classe ;\n",
    "- $\\hat \\pi_k$ est la proportion observée de la classe $k$, \n",
    "- $\\hat f_{k,j} $ est la densité gaussienne univariée de la classe $k$ pour la variable $j$. Les paramètres de cette loi valent (ajustés par maximum de vraisemblance) :\n",
    "    - $\\hat \\mu_{k,j}$ : la moyenne empirique de la variable $X^j$ restreinte à la classe k,\n",
    "    - $ \\hat \\sigma^2_{k,j}$ : la variance empirique de la variable $X^j$ restreinte à la classe k.\n",
    "    \n",
    "Noter que la fonction $x \\mapsto  \\prod_{j=1} ^6  f_{k,j}(x_j) $ peut aussi être vue comme une densité gaussienne multidimensionnelle de moyenne $(\\mu_{k,1}, \\dots, \\mu_{k,6})$ et de matrice de covariance diagonale $diag(\\hat \\sigma^2_{k,1},\\dots,\\hat  \\sigma^2_{k,6})$. Cette remarque évite de devoir calculer le produit de 6 densités univariées, à la place on calcule plus directement la valeur de la densité multidimensionnelle.\n",
    "\n",
    "Pour calculer la valeur de la densité d'une gaussienne multidimensionnelle en un point $x$ de $\\mathbb R ^d$ on peut utililser la fonction [`multivariate_normal`](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.multivariate_normal.html) de la librairie [`scipy.stats`](https://docs.scipy.org/doc/scipy/reference/stats.html). \n",
    "\n",
    "On pourra utiliser la fonction `var` de numpy pour calculer le vecteur des variances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul des moyennes et des variances de chaque variable pour chacun des deux groupes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_AB = np.mean(VertebralVar_train_AB, axis = 0)\n",
    "mean_NO = np.mean(VertebralVar_train_NO, axis = 0)\n",
    "\n",
    "# variances estimées variable par variable pour AB (sur le train) :\n",
    "var_AB = np.var(VertebralVar_train_AB, axis = 0)\n",
    "# variances estimées variable par variable pour NO (sur le train) :\n",
    "var_NO = np.var(VertebralVar_train_NO, axis = 0)\n",
    "\n",
    "# on forme les matrices de covariance (matrices diagonales car indep) :\n",
    "Cov_NB_AB =  np.diag(var_AB)\n",
    "Cov_NB_NO =  np.diag(var_NO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul du \"score\" sur chaque groupe pour chaque element des données test : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores_AB: 5.757446295461836e-127\n",
      "scores_NO: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.stats import multivariate_normal\n",
    "import numpy as np\n",
    "\n",
    "# Créer un modèle Bayesien naïf gaussien\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Utiliser les données d'entraînement pour ajuster le modèle\n",
    "gnb.fit(VertebralVar_train, VertebralClas_train)\n",
    "\n",
    "# Utiliser le modèle pour faire des prédictions de classe sur les données de test\n",
    "pred_NB_test = gnb.predict(VertebralVar_test)\n",
    "\n",
    "# Obtenez le nombre d'échantillons des catégories \"AB\" et \"NO\" dans les données d'entraînement\n",
    "num_samples_AB = np.sum(VertebralClas_train == 'AB')\n",
    "num_samples_NO = np.sum(VertebralClas_train == 'NO')\n",
    "\n",
    "\n",
    "# Calculer la probabilité a priori pi_k\n",
    "pi_AB = num_samples_AB / len(VertebralClas_train)\n",
    "pi_NO = num_samples_NO / len(VertebralClas_train)\n",
    "\n",
    "x_test = VertebralVar_test\n",
    "# Calculer le score des données de test score_NB_test\n",
    "\n",
    "scores_AB = pi_AB * np.prod([multivariate_normal.pdf(x_test[i], mean=mean_AB[i], cov=Cov_NB_AB[i, i]) for i in range(6)])\n",
    "scores_NO = pi_NO * np.prod([multivariate_normal.pdf(x_test[i], mean=mean_NO[i], cov=Cov_NB_NO[i, i]) for i in range(6)])\n",
    "\n",
    "print(\"scores_AB:\", scores_AB)\n",
    "print(\"scores_NO:\", scores_NO)\n",
    "# Sélectionnez la catégorie avec le score le plus élevé comme résultat de la prédiction\n",
    "score_NB_test = np.where(scores_AB > scores_NO, scores_AB, scores_NO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice de confusion est une matrice qui synthétise les performances d'une régle de classification. Chaque ligne correspond à une classe réelle, chaque colonne correspond à une classe estimée. La cellule (ligne L, colonne C) contient le nombre d'éléments de la classe réelle L qui ont été estimés comme appartenant à la classe C. Voir par exemple [ici](https://fr.wikipedia.org/wiki/Matrice_de_confusion).\n",
    "\n",
    "> Evaluer les performances de la méthode sur l'échantillon test. Vous pourrez utiliser la fonction [`confusion_matrix`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix) de la librairie [`sklearn.metrics`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Unnormalized):\n",
      " [[59 17]\n",
      " [ 9 31]]\n",
      "\n",
      "Confusion Matrix (Normalized):\n",
      " [[0.77631579 0.22368421]\n",
      " [0.225      0.775     ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculer la matrice de confusion\n",
    "cnf_matrix_NB_test = confusion_matrix(VertebralClas_test, pred_NB_test)\n",
    "\n",
    "# Calculer la matrice de confusion normalisée (exprimée en pourcentage)\n",
    "cnf_matrix_NB_test_normalized = cnf_matrix_NB_test.astype('float') / cnf_matrix_NB_test.sum(axis=1).reshape(-1,1) \n",
    "\n",
    "# Imprimer la matrice de confusion et la matrice de confusion normalisée\n",
    "print(\"Confusion Matrix (Unnormalized):\\n\", cnf_matrix_NB_test)\n",
    "print(\"\\nConfusion Matrix (Normalized):\\n\", cnf_matrix_NB_test_normalized)\n",
    "\n",
    "\n",
    "# cnf_matrix_NB_test.astype('float') / cnf_matrix_test.sum(axis=1).reshape(-1,1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  Il existe bien sûr une fonction scikit-learn  pour la méthode Naive Bayes : voir [ici](http://scikit-learn.org/stable/modules/naive_bayes.html). Vérifier que votre prédicteur donne la même réponse de cette fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[59 17]\n",
      " [ 9 31]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Use training data to fit the model\n",
    "gnb.fit(VertebralVar_train, VertebralClas_train)\n",
    "\n",
    "# Use the model to make class predictions on test data\n",
    "pred_NB_test = gnb.predict(VertebralVar_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cnf_matrix_NB_test = confusion_matrix(VertebralClas_test, pred_NB_test)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", cnf_matrix_NB_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifieur par plus proches voisins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est préférable d'utiliser la structure de données de type [k-d tree](https://en.wikipedia.org/wiki/K-d_tree) pour effectuer des requêtes de plus proches voisins dans un nuage de points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Contruction du k-d tree pour les données train (pour la métrique euclidienne) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "tree =  KDTree(VertebralVar_train, leaf_size=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Rechercher les 10 plus proches voisins dans les données d'apprentissage du premier point des données de test et afficher les classes de ces observations voisines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 8.08523964,  9.61650664, 11.4683957 , 12.09108763, 13.6653723 ,\n",
      "        14.83967318, 15.50358346, 18.32258988, 18.88993912, 19.09660703]]), array([[ 26,   2,  77, 116, 118,  70, 126, 137, 110, 159]], dtype=int64))\n",
      "['AB' 'AB' 'AB' 'AB' 'AB' 'NO' 'AB' 'AB' 'AB' 'AB']\n"
     ]
    }
   ],
   "source": [
    "indices_voisins =  tree.query(VertebralVar_test[0].reshape(1, -1), k=10)\n",
    "print(indices_voisins)\n",
    "classes_voisins = VertebralClas_train[indices_voisins[1][0]]\n",
    "print(classes_voisins)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le classifieur par plus proches vosins, la prediction est la classe majoritaire des k plus proches voisins.\n",
    "\n",
    "> Donner la prédiction pour le premier point de test par vote majoritaire sur ses 10 plus proches voisins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédiction pour le premier point de test : AB\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Obtenez les indices des 10 voisins les plus proches du premier point de données dans les données de test\n",
    "indices_voisins = tree.query(VertebralVar_test[0].reshape(1, -1), k=10)[1][0]\n",
    "\n",
    "# Obtenez les catégories de ces voisins\n",
    "classes_voisins = VertebralClas_train[indices_voisins]\n",
    "\n",
    "# Compter les occurrences de chaque catégorie\n",
    "count_classes = Counter(classes_voisins)\n",
    "\n",
    "# Trouvez les catégories les plus fréquentes\n",
    "prediction = count_classes.most_common(1)[0][0]\n",
    "\n",
    "# Imprimer les résultats de prédiction\n",
    "print(\"Prédiction pour le premier point de test :\", prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Donner la prediction du classifieur ppv pour toutes les données de test. Evaluer la qualité du classifieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[66 10]\n",
      " [ 7 33]]\n",
      "\n",
      "Normalized Confusion Matrix:\n",
      " [[0.86842105 0.13157895]\n",
      " [0.175      0.825     ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Choisissez une valeur k, qui est le nombre de voisins les plus proches\n",
    "k_class = 10 \n",
    "\n",
    "# Initialiser le tableau des résultats de prédiction\n",
    "pred_kNN_test = []\n",
    "\n",
    "# Faire des prédictions pour chaque point de données dans les données de test\n",
    "for i in range(len(VertebralVar_test)):\n",
    "    # Obtenez les indices des k voisins les plus proches du i-ème point de données dans les données de test\n",
    "    indices_voisins = tree.query(VertebralVar_test[i].reshape(1, -1), k=k_class)[1][0]\n",
    "    \n",
    "    # Obtenez les catégories de ces voisins\n",
    "    classes_voisins = VertebralClas_train[indices_voisins]\n",
    "    \n",
    "    # Compter les occurrences de chaque catégorie\n",
    "    count_classes = Counter(classes_voisins)\n",
    "    \n",
    "    # Trouvez la catégorie avec le plus d'occurrences comme résultat de la prédiction\n",
    "    prediction = count_classes.most_common(1)[0][0]\n",
    "    \n",
    "    # Ajouter les résultats de prédiction au tableau de prédiction\n",
    "    pred_kNN_test.append(prediction)\n",
    "\n",
    "# Calculer la matrice de confusion\n",
    "cnf_matrix_kNN = confusion_matrix(VertebralClas_test, pred_kNN_test)\n",
    "\n",
    "# Calculer la matrice de confusion normalisée\n",
    "cnf_matrix_kNN_normalized = cnf_matrix_kNN.astype('float') / cnf_matrix_kNN.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Imprimer la matrice de confusion\n",
    "print(\"Confusion Matrix:\\n\", cnf_matrix_kNN)\n",
    "print(\"\\nNormalized Confusion Matrix:\\n\", cnf_matrix_kNN_normalized)\n",
    "\n",
    "\n",
    "\n",
    "# k_class = ### CHOISIR  ####  #nombre de plus proche voisins utilisés\n",
    "# pred_kNN_test =  ### TO DO ####\n",
    "# cnf_matrix_kNN =### TO DO ####\n",
    "# cnf_matrix_kNN.astype('float') / cnf_matrix_kNN.sum(axis=1).reshape(-1,1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe bien sûr une fonction scikit-learn pour le classifieur plus proche voisin, voir [ici](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
